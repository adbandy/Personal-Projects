# Personal Projects

#### In general projects become more complex further down this page:


1) [Profitable App Profiles](https://github.com/adbandy/Personal-Projects/blob/main/Profitable%20App%20Profiles.ipynb)
A project to help developers understand what types of Apps attract the most users to determine lucrative markets.
  
	> Coding featured: **Python**, data cleaning, frequency tables
	
2) [Hacker News Posting Trends](https://github.com/adbandy/Personal-Projects/blob/main/Hacker%20News%20Posting%20Trends.ipynb)
A short project to work out the best times to post in order to get feedback comments from the 'Hacker News' site.
  
	> Coding featured: **Python**, loops, datetime
	
3) [Exploring Ebay Car Sales Data](https://github.com/adbandy/Personal-Projects/blob/main/Exploring%20Ebay%20Car%20Sales%20Data.ipynb)
This was a project reviewing the Ebay Car sales from the German Ebay classifieds. 

	> Coding featured: **Python**, data cleaning, pandas, numpy, groupby, unstack, boolean filters,  
	
4) [Visualising Earnings Based On College Majors](https://github.com/adbandy/Personal-Projects/blob/main/Visualizing%20Earnings%20Based%20On%20College%20Majors.ipynb)
 This project is a review of the database from the American Community Survey which documents the job outcomes of students who graduated between 2010 and 2012.
 
	> Coding featured: **Python**, data cleaning, graphical plots, histograms, scatter matrix, barh graphs  
	
5) [Visualizing The Gender Gap In College Degrees](https://github.com/adbandy/Personal-Projects/blob/main/Visualizing%20The%20Gender%20Gap%20In%20College%20Degrees.ipynb)
This visualisation project explores the differences between Men and Women in the database from The Department of Education Statistics containing the percentage of bachelor's degrees granted to women from 1970 to 2012.

	> Coding featured: **Python**, graphical plots, subplots, a clean graphical look including cleaning spines and updating ticks   
	
6) [Clean And Analyze Employee Exit Surveys](https://github.com/adbandy/Personal-Projects/blob/main/Clean%20And%20Analyze%20Employee%20Exit%20Surveys.ipynb)
This was a data cleaning project to review dissatisfaction in exit surveys in Australia. 

	> Coding featured: **Python**, data cleaning, pandas, numpy, vectorization methods (apply(), map(), fillna(), dropna() melt(), concat() ) groupby, unstack, boolean filters,      
	
7) [SAT correlations in NYC Schools](https://github.com/adbandy/Personal-Projects/blob/main/Analyzing%20NYC%20High%20School%20Data.ipynb)
A project that reviews the correlations in SATs in NYC schools with Race, Gender, Location, and Surveys from Teachers, Students and Parents.
  
	> Coding featured: **Python**, data cleaning, pandas, numpy, merge, maps (mpl_toolkits.basemap), graphical plots     
	
8) [Studying the Star Wars Survey](https://github.com/adbandy/Personal-Projects/blob/main/Studying%20the%20Star%20Wars%20Survey.ipynb)
A project reviewing a Star Wars Survey and uncovering trends.

	> Coding featured: **Python**, data cleaning, frequency tables, graphical plots      
	
9) [Analysing the CIA Factbook in SQL](https://github.com/adbandy/Personal-Projects/blob/main/Analysing%20The%20CIA%20Factbook%20Using%20SQL.ipynb)
The CIA factbook host interesting facts of countries around the world. In this project I used SQL to explore the dataset.

	> Coding featured: **SQL**, queries and sub-queries       
	
10) [Answering Business Questions In SQL](https://github.com/adbandy/Personal-Projects/blob/main/Answering%20Business%20Questions%20In%20SQL.ipynb)
This was an indepth project using SQL and its features to explore viable buisness options from the Chinook database. This contains 11 tables and is manipulated in SQL. 

	> Coding featured: **SQL**, queries, subqueries, create_table, joins and except, cast         
	
11) [Finding The Best Markets For An Online Learing Platform](https://github.com/adbandy/Personal-Projects/blob/main/Finding%20The%20Best%20Markets.ipynb)
This introductory statistics project look aims to determine the best market and finding anomalies using median, mode, mean comparisons

	> Coding featured: **Python**, pandas, Statistics including mean, median and mode, Seaborn boxplots and histograms   
	      
12) [A Mobile App For Lottery Addiction](https://github.com/adbandy/Personal-Projects/blob/main/Mobile%20App%20for%20Lottery%20Addiction.ipynb)
This statistics project involves the creation of multiple functions to display in fun and informative ways: the odds of winning big or small prizes; if previous winners match the ticket; likelyhood of repeats in given number of draws; and converting number of tickets bought to probability.

	> Coding featured: **Python**, pandas, Statistics including factorials, permutations/ combinations.
	
13) [Time Series Predictions on S&P Stock Market](https://github.com/adbandy/Personal-Projects/blob/main/Time%20Series%20Predictions%20on%20S%26P%20Stock%20Market.ipynb)
This time series project involves an investigaton to see if and which historical metrics can be used for predicting future closing trading prices of the S&P500 using linear regression machine learning

	> Coding featured: **Python**, Sklearn (linear regression, mean squared error) , time series statistics using .rolling()             
	
14) [Building a Spam Filter with Naive Bayes](https://github.com/adbandy/Personal-Projects/blob/main/Spam%20Filter.ipynb)
This statistics project involves the creation of a Spam Filter using Bayesian statisics. The model is created and test from an SMS database and gives a high accuracy

	> Coding featured: **Python**, pandas, Statistics including Naive Bayes statistics
	
15) [Peer to Peer Credit Modelling](https://github.com/adbandy/Personal-Projects/blob/main/Credit%20Modelling%20(1).ipynb)
This was a deep feature engineering project exploring credit risk from the lending club covering the whole machine learning workflow including class imbalance, redundant features, missing data, object to numerical conversion, get_dummies (one hot encoding) and various Classification machine learning

	> Coding featured: **Python**, Feature Engineering, Sklearn (Classifiers: K-Nearest Neighbours, Random Forests, ADABoost, GaussainNB, QDA all with K-Fold validation), pandas, numpy
	
16) [Predicting Bike Rentals With Random Forest Regression](https://github.com/adbandy/Personal-Projects/blob/main/Predicting%20Bike%20Rentals%20With%20Random%20Forest%20Regression.ipynb)
This machine learning project reviews data from Washington DC bike rentals. 3million rentals across 2 years. Linear regression, decision trees and random forest methods are compared following feature engineering of the dataset.

	> Coding featured: **Python**, Feature Engineering, Sklearn (LinearRegression, DecisionTreeRegressor, RandomForestRegressor, mean_squared_error), numpy. 

17) [Building a Handwritten Digits Classifier with Deep Feedforward Neural Networks](https://github.com/adbandy/Personal-Projects/blob/main/Building%20a%20Handwritten%20Digits%20Classifier%20with%20Deep%20Feedforward%20Neural%20Networks.ipynb)
This machine learning project involved the creation of Neural Networks with varying neurons and depths for predictions using a database of 1800 handwritten numbers. Accuracy was 96% using Neural Network compared to 

	> Coding featured: **Python**, Sklearn (K-Nearest Neighbours Classifier, MLPClassifier (Neural Network),  K-Fold validation variable fold), pandas.  

18) [Predicting Car Prices With K-Nearest Neighbours Machine Learning](https://github.com/adbandy/Personal-Projects/blob/main/Predicting%20Car%20Prices.ipynb)
This machine learning project involved the creation of various KNN functions to achieve the most accurate model for prediciting car prices from 13 features.

	> Coding featured: **Python**, Sklearn (Univariate KNN with fixed and variable neighbours, Multivariate KNN with fixed and variable neighbours, K-Fold validation variable fold and neighbours), pandas, seaborn.  

19) [Predicting House Prices With Linear Regression and K-Fold cross validation](https://github.com/adbandy/Personal-Projects/blob/main/Predicting%20House%20Sale%20Prices.ipynb)
This machine learning project involved in-depth Feature Engineering. This included calculating most correlating features, coalescing multiple features to more meaningful information (such as Year Sold - Year Build = Years until sold) removing data 'leaks' (information about target in other features) and use of get_dummies method to transform categorical features to numerical.

	> Coding featured: **Python**, Feature Engineering (including get_dummies), Data Cleaning, Sklearn (Linear Regression, K-Fold validation variable fold), numpy  

20) [Predicting Survival On The Titanic - A Kaggle Competition](https://github.com/adbandy/Personal-Projects/blob/main/Titanic.ipynb)
Using classification techniques (linear regression, KNN, and random forests) with preprocessing and model selection techniques (minmax scaling, RFECV, GridSearchCV) predictions were entered into Kaggle (of which the true results are never revealed).
The accuracy was increased from a position at 40000th to 2300th, with a final accuracy of 78.5%.

	> Coding featured: **Python**, Feature Engineering (including min_max scaling, binning, get_dummies RFECV, GridSearchCV), Data Cleaning, Sklearn (Cross validation,Logistical Regression, K-Fold validation variable fold, RandomForestClassifier), matplotlib, numpy.
